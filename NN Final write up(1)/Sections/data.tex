First we will start by creating our data sub-module, which will convert Wav files on disk to torch tensors, prepare and trim them, and then put them in a dataloader. 

One of the key problems in dissecting this implementation  is in understanding the structure of the data it takes in and returns. To put simply, WaveNet must take in sections of audio files that are longer than its receptive field, which is the length of audio it collapses to predict the next sample. The prediction targets and the output of the model are the section of audio starting at the first sample after the receptive field to the end of the file. This is because the model acts broadly as a convolutional network with a kernel size of the receptive field, passing over the audio and giving an output of size $file\_length - receptive\_field$.

This snippet below imports all of the modules we will need to create our data sub-module and defines a load function, which defers to librosa's load function to import the raw audio as a Numpy array. It throws away the extra dimension and optionally trims the file into sections of the given length. 

\newpage


\begin{minted}{python}
import os
import librosa
import numpy as np
import torch
import torch.utils.data as d

def load(filename, sample_rate=16000, trim=2048):
    '''
    filename (string): Path to audio file
    sample_rate (int): Sample rate to import file with
    Trim (int): number of samples in each clip, 0 returns whole file

    returns:
        audio     (NP array) : shape (trim,)
          OR
        raw_audio (NP array) : shape (n,)
    '''
    # Load in the raw audio using librosa and reshape to remove extra dimension
    # Underscore is the sample rate, which we already know
    raw_audio, _ = librosa.load(filename, sr=sample_rate, mono=True)
    raw_audio = raw_audio.reshape(-1, 1)

    # If requested, trim the clips of the specified number of samples
    if trim > 0:
        audio, _ = librosa.effects.trim(raw_audio, frame_length=trim)
        return audio

    return raw_audio
    
\end{minted}

What does this returned data look like? If we run the following snippet, we can get an idea:

\begin{minted}{python}
>>> x = load('./data/helloworld.wav')
>>> x
array([[ 3.0305667e-09],
       [ 2.7823888e-10],
       [-4.6928004e-09],
       ...,
       [ 0.0000000e+00],
       [ 0.0000000e+00],
       [ 0.0000000e+00]], dtype=float32)
>>> x.shape
(1668550, 1)
\end{minted}

Just as we wanted, our function returns a one dimensional NumPy array of the raw audio. However, we're not done yet. We still need to decide what our targets are, and before we can do that, we need to adjust the values of the array to something our model can work with more accurately. At the end of our process we use a softmax distribution to predict the next sample, which will perform poorly if trying to estimate for the 65536 possible values of a 16 bit audio file. To remedy will will use Mu-law companding, as recommended by the original paper, to compress the range down to 256 possible values. Unfortunately, this is a lossy compression that will reduce our bit depth down to 8. The following shows the formula to achieve this:

\[f(x_t)=\sign(x_t)\frac{\ln(1+\mu|x_t|)}{\ln(1+\mu)}\]

\newpage 

\begin{minted}{python}
def mu_law_encode(waveform, channels=256):
    '''
    Quantize waveform amplitudes
    '''
    # create an array of equally spaced points between -1 and 1
    lin_space = np.linspace(-1, 1, channels)
    # Apply the formula
    channels = float(channels)
    quantized = np.sign(waveform) * np.log(1 + (channels - 1) * np.abs(waveform)) / np.log(channels)
    # Discretize and return
    return np.digitize(quantized, lin_space) - 1
    
>>> x = load('./data/helloworld.wav')
>>> mu_law_encode(x)
array([[127],
       [127],
       [127],
       ...,
       [127],
       [127],
       [127]])
\end{minted}

After quantizing, we create a one-hot encoding to convert a complicated regression task into a simpler classification task.

\begin{minted}{python}
def one_hot_encode(data, channels=256):
    '''
    Creates a one-hot encoding of the 1D input data
    returns np array of size (data.size, channels)
    '''
    one_hot = np.zeros((data.size, channels), dtype=float)
    # Make the value one at the the column corresponding to the row value
    one_hot[np.arange(data.size), data.ravel()] = 1

    return one_hot
\end{minted}

Finally, we need functions to convert our encoding and companding back to raw audio. The formula to convert our Mu law companding back to floats is the following:

\[f^{-1}(y_t)=\sign(y_t)(1/\mu)((1+\mu)^{|y_t|}-1\]

\begin{minted}{python}
def one_hot_decode(data, axis=1):
    '''
    Decodes a one-hot encoding
    returns a 1D array
    '''
    return np.argmax(data, axis=axis)
    
def mu_law_decode(data, channels=256):
    '''
    Recovers the waveform from discretized data
    '''
    channels = float(channels)
    # re-centers the data around 0
    exp = -1 + (data / channels) *2.0
    return np.sign(exp) * (np.exp(np.abs(exp) * np.log(channels)) - 1) / (channels - 1)
\end{minted}

Now that we've got all of our importing and translation functions, we can create a custom dataset and dataloader. To create a dataset, we need to define two methods: one that gets the item at a given index, and one that returns the length of the dataset. Ours will take in the path to a directory of audio files and for each index return an encoded version of the whole file, which our dataloader will divide up into pieces for analysis.

Our model won't use this directly, but our dataloader will use it behind the scenes\footnote{The code for the \code{CustomDataset} class is available in Appendix A}. To create a dataloader, we need to define a function that collates a given file into a generator of trimmed sections, shown below. This function is called while looping through the dataloader, which automatically prepends a batch dimension. We have two hidden helper methods not shown: one that calculates the sample size and one that converts our NumPy array to a torch tensor using the GPU if it is available.

\begin{minted}{python}
def _collate_fn(self, audio):
    # First pad the audio along the timestep dimension to make sure its longer
    # than the required sample size, it gets cut down later
    audio = np.pad(audio, [[0, 0], [self.receptive_field, 0], [0, 0]], 'constant')

    if self.sample_size:
        # If a sample size greater than 0 is given, break the file into
        # sections that are that long
        sample_size = self._sample_size(audio)

        # This while loop breaks the waveform up into chunks and returns them
        # one at a time
        while sample_size > self.receptive_field:
            inputs = audio[:, :sample_size, :]
            targets = audio[:, self.receptive_field:sample_size, :]

            yield self._variable(inputs), self._variable(one_hot_decode(targets, 2))

            audio = audio[:, sample_size - self.receptive_field:, :]
            sample_size = self._sample_size(audio)
    else:
        # if the sample size is zero or None or False, return the whole file
        targets = audio[:, self.receiptive_field:, :]
        return self._variable(audio), self._variable(one_hot_decode(targets, 2))
\end{minted}

Now that we have our dataloader, we can package all of this into a sub-module that we can import in our next adventure.

