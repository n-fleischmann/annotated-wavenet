With the computing power available, we built and trained a model that can recognize and imitate broad features like repetitive rhythms in audio files. While the results may not appear impressive at first pass, the fact this is at all possible using 317 audio clips and a GTX 1660 Super is a feat.  An obvious next step would be to train a more complicated model for longer, which might give results similar to the samples in the original WaveNet blog post. There are also a number of other features still left to implement, like combining this with a text-to-sequence model for a full text-to-speech application, or including interchangeable speaker embedding vectors. This paper has just scratched the surface getting the model up and running. The modular approach allows for the expansion of this architecture into whatever creativity and computational complexity allows.