At the time of its release, WaveNet was revolutionary. Particularly with its pre-trained model, an architecture that allowed for audio-to-audio sequencing was available for specialization with relatively small amounts of time and computing power. Still nearly four years later, information on how WaveNet works in detail is thin. The aim of this paper is to fill the gap between the abstract description of the original paper and the undocumented versions one might find on GitHub.

This paper is a deep dive into a PyTorch implementation by Jaehun Ryu on github, which can be found \href{https://github.com/ryujaehun/wavenet}{here}. A full list of the modules we will use can be found in \code{requirements.txt}. In particular, torchaudio, librosa, and soundfile are key workhorses that do not come standard with most environments. If following along on any Linux operating system, ffmpeg is also required as an audio backend. 